{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFFicbuQllu88IZqMIPD+2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/RL-Q-Learning-from-scratch/blob/main/RL_Q_learning_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tOwRcI_F6qyl"
      },
      "outputs": [],
      "source": [
        "import random # provides functions to generate random numbers\n",
        "from typing import List # This imports the List type hint from the typing module.\n",
        "                        # It is used for type annotations, allowing you to specify that a function parameter or return value should be a list of a certain type\n",
        "                        # (e.g., List[int] means a list of integers)."
      ]
    },
    {
      "source": [
        "class SampleEnvironment:\n",
        "  def __init__(self): # Changed _init_ to __init__\n",
        "    self.steps_left = 20  # tells agents how is the game\n",
        "\n",
        "  def get_observation(self) -> List[float]:\n",
        "    return [0.0, 0.0, 0.0] # present position in environment [ not 0.0 any value is valid here]\n",
        "\n",
        "  def get_actions(self) -> List[int]:\n",
        "    return [0, 1] # possible actions an agent can make and by agent we simply means a entity that implements policy\n",
        "\n",
        "  def is_done(self) -> bool:\n",
        "    return self.steps_left == 0 # tells the agent game is done where no steps were left\n",
        "\n",
        "  def action_reward(self, action: int) -> float:\n",
        "    if self.is_done():                   # checks if game is over or not\n",
        "      raise Exception(\"Game is over\")   # if not\n",
        "    self.steps_left -= 1                # decrease the steps\n",
        "    return random.random()              # and return a reward value for action"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OKhUJ9KPBiFf"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "source": [
        "class Agent:\n",
        "  def __init__(self):         # when agent is created it's reward value is 0\n",
        "    self.total_reward = 0.0\n",
        "\n",
        "  def step(self, env: SampleEnvironment):\n",
        "    current_obs = env.get_observation()          # observe the environment i.e, here it observes its position in env\n",
        "    print(\"Observation {}\".format(current_obs))  # prints observation ( but here we've hard coded it for a specific value but in real life agent gets live value )\n",
        "    actions = env.get_actions()                  # gets actions ( but here we've hard coded it randomly but in real life agent does this by observation values and a policy )\n",
        "    print(actions)\n",
        "    reward = env.action_reward(random.choice(actions))    # gets a random reward\n",
        "    self.total_reward += reward                          # aggregate it to total reward\n",
        "    print(\"Total reward {}\".format(self.total_reward))   # prints total reward\n",
        "                         # The .format() method replaces {} with the value of self.total_reward."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IiIeyve6BxWQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SampleEnvironment() # enviroment instance\n",
        "agent = Agent()           # agents instance\n",
        "i = 0\n",
        "\n",
        "\n",
        "while not env.is_done():   # environment steps are not done then perfomr some actions\n",
        "  i=i+1\n",
        "  print(\"Step {}\".format(i))\n",
        "  agent.step(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tT6sJWe17ubT",
        "outputId": "8f8ecba8-d32c-45ed-8f46-15c7d4217e82"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 0.6872604108496245\n",
            "Step 2\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 1.5467820931071996\n",
            "Step 3\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 2.53854296927009\n",
            "Step 4\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 2.7969451047873064\n",
            "Step 5\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 3.3554758935715103\n",
            "Step 6\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 4.094555586877653\n",
            "Step 7\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 4.108216790035494\n",
            "Step 8\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 5.0755677359436895\n",
            "Step 9\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 5.344112106510347\n",
            "Step 10\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 5.637863780567108\n",
            "Step 11\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 5.9738660843981615\n",
            "Step 12\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 6.036383115339015\n",
            "Step 13\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 6.5095555384568655\n",
            "Step 14\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 6.559586842327592\n",
            "Step 15\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 7.053508123596858\n",
            "Step 16\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 7.683608307197374\n",
            "Step 17\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 7.91566741226161\n",
            "Step 18\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 7.99793038337177\n",
            "Step 19\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 8.81889085070396\n",
            "Step 20\n",
            "Observation [0.0, 0.0, 0.0]\n",
            "[0, 1]\n",
            "Total reward 9.217865862737755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code does the same function of RL but with random policy"
      ],
      "metadata": {
        "id": "Q2UK1hsXIGEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's Explore the same code but with optimised policy of: Q-Learning"
      ],
      "metadata": {
        "id": "aZMTh0w3IWa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import List\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ExFl7NTnCJbx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleEnvironment:\n",
        "  def __init__(self):\n",
        "    self.steps_left = 20\n",
        "\n",
        "  def get_observation(self) -> List[float]:\n",
        "    return [0.0, 0.0, 0.0]\n",
        "\n",
        "  def get_actions(self) -> List[int]: # from there [0,1] binary set of actions agent will pick one\n",
        "    return [0, 1]\n",
        "\n",
        "  def is_done(self) -> bool:\n",
        "    return self.steps_left == 0\n",
        "\n",
        "  def action_reward(self, action: int) -> float:\n",
        "    if self.is_done():\n",
        "      raise Exception(\"Game is over\")\n",
        "    self.steps_left -= 1\n",
        "    return random.random()"
      ],
      "metadata": {
        "id": "JYgY-kfcIqog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, learning_rate=0.1, discount_factor=0.9):\n",
        "    self.total_reward = 0.0\n",
        "    self.q_table = {}  # Initialize Q-table as a dictionary\n",
        "    self.learning_rate = learning_rate\n",
        "    self.discount_factor = discount_factor\n",
        "\n",
        "  def get_action(self, env: SampleEnvironment, epsilon=0.1):\n",
        "    \"\"\"Choose an action using an epsilon-greedy policy. for balancing exploration and exploitation i.e, balance exploration with maximum profit path both- explained in-depth at end \"\"\"\n",
        "    actions = env.get_actions() # Get possible actions: [0,1]\n",
        "\n",
        "    # Explore: Choose a random action (10% chance by default)\n",
        "    if random.uniform(0, 1) < epsilon: # This code generates a random floating-point number between 0 and 1, and checks if it's less than epsilon\n",
        "      return random.choice(actions)\n",
        "\n",
        "    else:\n",
        "      # Exploit: Choose the action with the highest Q-value\n",
        "      current_obs = tuple(env.get_observation()) # Convert state to tuple (for dictionary key)\n",
        "\n",
        "      if current_obs not in self.q_table:\n",
        "        # If the state is not in Q-table, initialize Q-values for all actions\n",
        "        self.q_table[current_obs] = {action: 0.0 for action in actions}\n",
        "\n",
        "      action_values = self.q_table[current_obs] # Get Q-values for current state\n",
        "      return max(action_values, key=action_values.get) # choose best action from [0,1] from Q_table\n",
        "\n",
        "  def update_q_table(self, current_obs, action, reward, next_obs):\n",
        "    \"\"\"Update Q-table using the Q-learning algorithm.\"\"\"\n",
        "    current_obs = tuple(current_obs) # Convert state to tuple (dictionary key)\n",
        "    next_obs = tuple(next_obs)  # Convert next state to tuple\n",
        "\n",
        "    # Initialize Q-values for unseen states\n",
        "    if current_obs not in self.q_table:\n",
        "      self.q_table[current_obs] = {action: 0.0 for action in [0, 1]}\n",
        "\n",
        "    if next_obs not in self.q_table:\n",
        "      self.q_table[next_obs] = {action: 0.0 for action in [0, 1]}\n",
        "\n",
        "    # Find the best action for the next state\n",
        "    best_next_action = max(self.q_table[next_obs], key=self.q_table[next_obs].get)\n",
        "\n",
        "    # Q-learning formula:\n",
        "    td_target = reward + self.discount_factor * self.q_table[next_obs][best_next_action]\n",
        "    td_error = td_target - self.q_table[current_obs][action]\n",
        "    self.q_table[current_obs][action] += self.learning_rate * td_error\n",
        "\n",
        "    # SAMPLE Q-TALBE IS DISPLAYED AT END.\n",
        "\n",
        "  def step(self, env: SampleEnvironment):\n",
        "    current_obs = env.get_observation()\n",
        "    action = self.get_action(env)\n",
        "    print(\"Action {}\".format(action))\n",
        "    reward = env.action_reward(action)\n",
        "    print(\"Reward {}\".format(reward))\n",
        "    self.total_reward += reward\n",
        "    next_obs = env.get_observation()\n",
        "\n",
        "    self.update_q_table(current_obs, action, reward, next_obs)\n",
        "    print(\"Total reward {}\".format(self.total_reward))"
      ],
      "metadata": {
        "id": "HDxvWbQbIvEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SampleEnvironment()\n",
        "agent = Agent()\n",
        "i = 0\n",
        "\n",
        "while not env.is_done():\n",
        "  i += 1\n",
        "  print(\"Step {}\".format(i))\n",
        "  agent.step(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnbGyZXGIzox",
        "outputId": "bb3afb15-cfa8-4776-e524-2e6dcf6d1884"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action 0\n",
            "Reward 0.9792935513129654\n",
            "Total reward 0.9792935513129654\n",
            "Step 2\n",
            "Action 0\n",
            "Reward 0.8511365243821553\n",
            "Total reward 1.8304300756951206\n",
            "Step 3\n",
            "Action 0\n",
            "Reward 0.7520841381419586\n",
            "Total reward 2.5825142138370794\n",
            "Step 4\n",
            "Action 0\n",
            "Reward 0.3649034309418704\n",
            "Total reward 2.94741764477895\n",
            "Step 5\n",
            "Action 0\n",
            "Reward 0.39611983546424845\n",
            "Total reward 3.3435374802431985\n",
            "Step 6\n",
            "Action 0\n",
            "Reward 0.5722373211830409\n",
            "Total reward 3.9157748014262395\n",
            "Step 7\n",
            "Action 0\n",
            "Reward 0.04433717598054765\n",
            "Total reward 3.960111977406787\n",
            "Step 8\n",
            "Action 0\n",
            "Reward 0.536458008804654\n",
            "Total reward 4.4965699862114406\n",
            "Step 9\n",
            "Action 0\n",
            "Reward 0.7365899582090493\n",
            "Total reward 5.23315994442049\n",
            "Step 10\n",
            "Action 0\n",
            "Reward 0.8784716480873132\n",
            "Total reward 6.111631592507803\n",
            "Step 11\n",
            "Action 0\n",
            "Reward 0.891256359907843\n",
            "Total reward 7.002887952415646\n",
            "Step 12\n",
            "Action 0\n",
            "Reward 0.07701312755748069\n",
            "Total reward 7.079901079973127\n",
            "Step 13\n",
            "Action 0\n",
            "Reward 0.8997641043901781\n",
            "Total reward 7.979665184363306\n",
            "Step 14\n",
            "Action 0\n",
            "Reward 0.257586386942543\n",
            "Total reward 8.23725157130585\n",
            "Step 15\n",
            "Action 0\n",
            "Reward 0.22800955475773566\n",
            "Total reward 8.465261126063584\n",
            "Step 16\n",
            "Action 0\n",
            "Reward 0.7302274231594401\n",
            "Total reward 9.195488549223025\n",
            "Step 17\n",
            "Action 1\n",
            "Reward 0.503593797690035\n",
            "Total reward 9.69908234691306\n",
            "Step 18\n",
            "Action 0\n",
            "Reward 0.9918232504505639\n",
            "Total reward 10.690905597363624\n",
            "Step 19\n",
            "Action 0\n",
            "Reward 0.10168902221969445\n",
            "Total reward 10.79259461958332\n",
            "Step 20\n",
            "Action 0\n",
            "Reward 0.2807004346900718\n",
            "Total reward 11.073295054273391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proved Concept: randomly we are getting 9.2 and by Q-learning we are getting 11 { Values might vary on every run }"
      ],
      "metadata": {
        "id": "Gg2b-b1fGXlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "-Peoya09FRjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-Learning Algorithms"
      ],
      "metadata": {
        "id": "5FlrHxCZEyOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-learning is a model-free reinforcement learning algorithm that helps an agent learn optimal actions by updating a Q-table based on rewards. The agent follows an ε-greedy policy, where it explores by taking random actions with probability ε and exploits by selecting the action with the highest Q-value otherwise. The Q-value update rule is based on the Bellman equation.\n",
        "\n",
        "Where α (learning rate) controls update speed, γ (discount factor) determines future reward importance, and R is the immediate reward. The agent continuously updates Q-values based on observed state-action-reward transitions, refining its decision-making. Over time, Q-learning converges to an optimal policy, allowing the agent to maximize cumulative rewards."
      ],
      "metadata": {
        "id": "nYCvlFjUEzLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "J95_ki2PFPA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ε-Greedy Policy in Reinforcement Learning"
      ],
      "metadata": {
        "id": "JMuR03W9FJBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ε-greedy policy is a reinforcement learning strategy that balances exploration (trying new actions) and exploitation (choosing the best-known action). At each step, with probability ε, the agent selects a random action to explore, and with probability 1 - ε, it picks the action with the highest Q-value. This prevents the agent from getting stuck in suboptimal strategies while still allowing it to learn the best actions over time. A decaying ε (reducing exploration gradually) helps shift from learning to optimal decision-making. This approach ensures the agent maximizes long-term rewards while continuously refining its strategy."
      ],
      "metadata": {
        "id": "bf7OhykwFMrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why not E-greedy Always?"
      ],
      "metadata": {
        "id": "RsDxNlkQFwNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using ε-greedy all the time (with a fixed high ε) results in excessive random exploration, preventing the agent from fully exploiting what it has learned. Conversely, setting ε too low from the start leads to premature exploitation, causing the agent to get stuck in suboptimal strategies without discovering better actions."
      ],
      "metadata": {
        "id": "v6sbaaiZFrbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Q-Table for this Code"
      ],
      "metadata": {
        "id": "iQ0mqtxzH4Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  (0.0, 0.0, 0.0): {  # State (Observation)\n",
        "    0: 0.15,  # Q-value for taking action 0\n",
        "    1: 0.23   # Q-value for taking action 1\n",
        "  },\n",
        "  (1.0, 0.0, 0.0): {  # Another state (if observation changed)\n",
        "    0: 0.10,\n",
        "    1: 0.19\n",
        "  }\n",
        "}"
      ],
      "metadata": {
        "id": "aylHXqD9H7Ra"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}